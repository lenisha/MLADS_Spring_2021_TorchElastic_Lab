{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit"
  },
  "interpreter": {
   "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Run Distributed Training Job\n",
    "In this notebook we will\n",
    "- Deploy Torch Elastic Kubernetes components \n",
    "  - Torch Elastic Operator\n",
    "  - ETCD server for training control plane\n",
    "- Prepare and Deploy `ElasticJob` based on PyTorch Elastic ImageNet Training  \n",
    "- Validate Training is running on multiple GPU workers\n",
    "\n",
    "This notebook will deploy all components as per our architecure and run the training job \n",
    "![architecture](docs/architecture.jpg)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Deploy Torch Elastic Kubernetes components\n",
    "\n",
    "## ETCD server\n",
    "First we will install ETCD server whcih will act as Rendezevous server orchestrating training workers.\n",
    "In this example we use simple ETCD pod/service deployment, for production use Helm chart that deploy `ETCD` in HA mode."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "service/etcd-service unchanged\n",
      "pod/etcd unchanged\n",
      "NAME           TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)    AGE\n",
      "etcd-service   ClusterIP   10.0.216.198   <none>        2379/TCP   42s\n",
      "NAME   READY   STATUS    RESTARTS   AGE   IP            NODE                                 NOMINATED NODE   READINESS GATES\n",
      "etcd   1/1     Running   0          43s   10.244.12.2   aks-cpuworkers-40607851-vmss000000   <none>           <none>\n"
     ]
    }
   ],
   "source": [
    "!kubectl apply -f kube/etcd.yaml\n",
    "# verify service and pods scheduled on CPU nodes\n",
    "!kubectl get svc -n elastic-job\n",
    "!kubectl get pods -n elastic-job -o wide\n"
   ]
  },
  {
   "source": [
    "## Deploy TorchElastic Operator\n",
    "Details and Kubernetes manifests descibed at https://github.com/pytorch/elastic/tree/master/kubernetes, we have included manifests in the repo for simplicity:\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Warning: resource namespaces/elastic-job is missing the kubectl.kubernetes.io/last-applied-configuration annotation which is required by kubectl apply. kubectl apply should only be used on resources created declaratively by either kubectl create --save-config or kubectl apply. The missing annotation will be patched automatically.\n",
      "namespace/elastic-job configured\n",
      "customresourcedefinition.apiextensions.k8s.io/elasticjobs.elastic.pytorch.org created\n",
      "role.rbac.authorization.k8s.io/leader-election-role created\n",
      "clusterrole.rbac.authorization.k8s.io/elastic-job-k8s-controller-role created\n",
      "rolebinding.rbac.authorization.k8s.io/leader-election-rolebinding created\n",
      "clusterrolebinding.rbac.authorization.k8s.io/elastic-job-k8s-controller-rolebinding created\n",
      "deployment.apps/elastic-job-k8s-controller created\n"
     ]
    }
   ],
   "source": [
    "!kubectl apply -k kube/config/default"
   ]
  },
  {
   "source": [
    "# Verify that the ElasticJob custom resource is installed\n",
    "!kubectl get crd\n"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 6,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "NAME                              CREATED AT\nelasticjobs.elastic.pytorch.org   2021-05-24T00:48:59Z\n"
     ]
    }
   ]
  },
  {
   "source": [
    "## Prepare ElasticJob Deployment\n",
    "### Training Docker imagen\n",
    "Take a look at `Dockerfile` for our deployment, it is in [examples/Dockerfile](examples/Dockerfile).\n",
    "It is based on `pytorch` gpu enabled image and has both training script `main.py` and dataset for training and testing as part of it.\n",
    "In production dataset should reside in blob and deployment should point to it.\n",
    "\n",
    "Keep a not theta entrypoint in the container is launch\n",
    "```\n",
    "ENTRYPOINT [\"python\", \"-m\", \"torch.distributed.run\"]\n",
    "```\n",
    "For more details refer to https://github.com/pytorch/elastic/tree/master/examples\n",
    "\n",
    "## Kubernetes Job config\n",
    "We have updated Kubernetes `ElasticJob` manifest [kube/imagenet.yaml](kube/imagenet.yaml) to make sure it could run on Spot nodes and mount Azure Blob for saving checkpoints.\n",
    "\n",
    "- Note we are deploying Custom Kubernetes Resource `ElasticJob` that Torch Elastic operator will process and orchestrate\n",
    "- We pointed `rdzvEndpoint` to previously deployed ETCD service \n",
    "- Note min/max number of replicas that directs training job on number of desired workers, if you increase **number of workers** you would see that each worker is performing training on smaller subset of data and overall job completes much faster\n",
    "\n",
    "```yaml\n",
    "  apiVersion: elastic.pytorch.org/v1alpha1\n",
    "  kind: ElasticJob\n",
    "  metadata:\n",
    "    name: imagenet\n",
    "    namespace: elastic-job\n",
    "  spec:\n",
    "    # Use \"etcd-service:2379\" if you already apply etcd.yaml\n",
    "    rdzvEndpoint: \"etcd-service:2379\"\n",
    "    minReplicas: 1\n",
    "    maxReplicas: 3    \n",
    "    replicaSpecs:\n",
    "      Worker:\n",
    "        replicas: 3\n",
    "```\n",
    "\n",
    "- Updated `kube/imagenet.yaml` with **tolerations** and **nodeSelector** to run training on Spot VM nodepool\n",
    "\n",
    "```yaml\n",
    "    containers:\n",
    "    - name: elasticjob-worker\n",
    "      image: torchelastic/examples:0.2.0\n",
    "      imagePullPolicy: Always\n",
    "       ..\n",
    "    nodeSelector:\n",
    "       kubernetes.azure.com/scalesetpriority: spot\n",
    "    tolerations:\n",
    "    - key: \"kubernetes.azure.com/scalesetpriority\"\n",
    "      operator: \"Equal\"\n",
    "      value: \"spot\"\n",
    "      effect: \"NoSchedule\"       \n",
    "```\n",
    "\n",
    "- Updated `kube/imagenet.yaml` with **volumes** and **volumemount** to provide storage to the training job to save checkpoint to the path set in arguments `--checkpoint-file`\n",
    "\n",
    "  ```yaml\n",
    "  volumes:  \n",
    "  - name: trainingdata\n",
    "    persistentVolumeClaim:\n",
    "        claimName: pvc-blob\n",
    "  containers:\n",
    "    ...\n",
    "    args:\n",
    "    - \"--nproc_per_node=1\"\n",
    "    - \"/workspace/examples/imagenet/main.py\"\n",
    "    - \"--arch=resnet18\"\n",
    "    - \"--epochs=3\"\n",
    "    - \"--batch-size=64\"\n",
    "    - \"--workers=0\"\n",
    "    - \"/workspace/data/tiny-imagenet-200\"\n",
    "    - \"--checkpoint-file=/mnt/blob/data/checkpoint.pth.tar\"\n",
    "    volumeMounts:\n",
    "    - name: trainingdata\n",
    "      mountPath: \"/mnt/blob/data\"         \n",
    "  ```\n",
    "\n",
    "- Note in the arguments `--nproc_per_node` directs on how many local pytorch workers could run per node (typically is equal to number of CUDA devices), '--workers` is number of workers in Pytorch Dataloader \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "elasticjob.elastic.pytorch.org/imagenet created\n",
      "Name:         imagenet\n",
      "Namespace:    elastic-job\n",
      "Labels:       <none>\n",
      "Annotations:  <none>\n",
      "API Version:  elastic.pytorch.org/v1alpha1\n",
      "Kind:         ElasticJob\n",
      "Metadata:\n",
      "  Creation Timestamp:  2021-05-25T19:42:06Z\n",
      "  Generation:          2\n",
      "  Managed Fields:\n",
      "    API Version:  elastic.pytorch.org/v1alpha1\n",
      "    Fields Type:  FieldsV1\n",
      "    fieldsV1:\n",
      "      f:metadata:\n",
      "        f:annotations:\n",
      "          .:\n",
      "          f:kubectl.kubernetes.io/last-applied-configuration:\n",
      "      f:spec:\n",
      "        .:\n",
      "        f:maxReplicas:\n",
      "        f:minReplicas:\n",
      "        f:rdzvEndpoint:\n",
      "        f:replicaSpecs:\n",
      "          .:\n",
      "          f:Worker:\n",
      "            .:\n",
      "            f:replicas:\n",
      "            f:restartPolicy:\n",
      "            f:template:\n",
      "              .:\n",
      "              f:spec:\n",
      "                .:\n",
      "                f:nodeSelector:\n",
      "                  .:\n",
      "                  f:kubernetes.azure.com/scalesetpriority:\n",
      "                f:tolerations:\n",
      "                f:volumes:\n",
      "    Manager:      kubectl-client-side-apply\n",
      "    Operation:    Update\n",
      "    Time:         2021-05-25T19:42:06Z\n",
      "    API Version:  elastic.pytorch.org/v1alpha1\n",
      "    Fields Type:  FieldsV1\n",
      "    fieldsV1:\n",
      "      f:spec:\n",
      "        f:RunPolicy:\n",
      "          .:\n",
      "          f:cleanPodPolicy:\n",
      "        f:replicaSpecs:\n",
      "          f:Worker:\n",
      "            f:template:\n",
      "              f:metadata:\n",
      "                .:\n",
      "                f:creationTimestamp:\n",
      "              f:spec:\n",
      "                f:containers:\n",
      "      f:status:\n",
      "        .:\n",
      "        f:conditions:\n",
      "        f:replicaStatuses:\n",
      "          .:\n",
      "          f:Worker:\n",
      "    Manager:         manager\n",
      "    Operation:       Update\n",
      "    Time:            2021-05-25T19:42:06Z\n",
      "  Resource Version:  618984\n",
      "  Self Link:         /apis/elastic.pytorch.org/v1alpha1/namespaces/elastic-job/elasticjobs/imagenet\n",
      "  UID:               806c3172-ba28-455f-92aa-77bb20a0de45\n",
      "Spec:\n",
      "  Run Policy:\n",
      "    Clean Pod Policy:  None\n",
      "  Max Replicas:        3\n",
      "  Min Replicas:        1\n",
      "  Rdzv Endpoint:       etcd-service:2379\n",
      "  Replica Specs:\n",
      "    Worker:\n",
      "      Replicas:        3\n",
      "      Restart Policy:  ExitCode\n",
      "      Template:\n",
      "        Metadata:\n",
      "          Creation Timestamp:  <nil>\n",
      "        Spec:\n",
      "          Containers:\n",
      "            Args:\n",
      "              --nproc_per_node=1\n",
      "              /workspace/examples/imagenet/main.py\n",
      "              --arch=resnet18\n",
      "              --epochs=3\n",
      "              --batch-size=64\n",
      "              --workers=0\n",
      "              /workspace/data/tiny-imagenet-200\n",
      "              --checkpoint-file=/mnt/blob/data/checkpoint.pth.tar\n",
      "            Image:              torchelastic/examples:0.2.0\n",
      "            Image Pull Policy:  Always\n",
      "            Name:               elasticjob-worker\n",
      "            Resources:\n",
      "              Limits:\n",
      "                nvidia.com/gpu:  1\n",
      "            Volume Mounts:\n",
      "              Mount Path:  /mnt/blob/data\n",
      "              Name:        trainingdata\n",
      "          Node Selector:\n",
      "            kubernetes.azure.com/scalesetpriority:  spot\n",
      "          Tolerations:\n",
      "            Effect:    NoSchedule\n",
      "            Key:       kubernetes.azure.com/scalesetpriority\n",
      "            Operator:  Equal\n",
      "            Value:     spot\n",
      "          Volumes:\n",
      "            Name:  trainingdata\n",
      "            Persistent Volume Claim:\n",
      "              Claim Name:  pvc-blob\n",
      "Status:\n",
      "  Conditions:\n",
      "    Last Transition Time:  2021-05-25T19:42:06Z\n",
      "    Last Update Time:      2021-05-25T19:42:06Z\n",
      "    Message:               ElasticJob imagenet is running.\n",
      "    Reason:                ElasticJobRunning\n",
      "    Status:                True\n",
      "    Type:                  Running\n",
      "  Replica Statuses:\n",
      "    Worker:\n",
      "Events:\n",
      "  Type    Reason                   Age        From                    Message\n",
      "  ----    ------                   ----       ----                    -------\n",
      "  Normal  SuccessfulCreatePod      <invalid>  elastic-job-controller  Created pod: imagenet-worker-0\n",
      "  Normal  SuccessfulCreatePod      <invalid>  elastic-job-controller  Created pod: imagenet-worker-1\n",
      "  Normal  SuccessfulCreatePod      <invalid>  elastic-job-controller  Created pod: imagenet-worker-2\n",
      "  Normal  SuccessfulCreateService  <invalid>  elastic-job-controller  Created service: imagenet-worker-0\n",
      "  Normal  SuccessfulCreateService  <invalid>  elastic-job-controller  Created service: imagenet-worker-1\n",
      "  Normal  SuccessfulCreateService  <invalid>  elastic-job-controller  Created service: imagenet-worker-2\n"
     ]
    }
   ],
   "source": [
    "# Run the deployment\n",
    "!kubectl apply -f kube/imagenet.yaml\n",
    "# Verify ElasticJob\n",
    "!kubectl describe elasticjob -n elastic-job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "NAME                                          READY   STATUS    RESTARTS   AGE   IP            NODE                                 NOMINATED NODE   READINESS GATES\nelastic-job-k8s-controller-5b9bc6b79c-xvdsw   1/1     Running   0          42h   10.244.13.2   aks-cpuworkers-40607851-vmss000003   <none>           <none>\netcd                                          1/1     Running   0          42h   10.244.12.2   aks-cpuworkers-40607851-vmss000000   <none>           <none>\nimagenet-worker-0                             1/1     Running   0          5s    10.244.16.9   aks-spotgpu-40607851-vmss000000      <none>           <none>\nimagenet-worker-1                             1/1     Running   0          5s    10.244.18.6   aks-spotgpu-40607851-vmss000002      <none>           <none>\nimagenet-worker-2                             1/1     Running   0          5s    10.244.18.7   aks-spotgpu-40607851-vmss000002      <none>           <none>\n"
     ]
    }
   ],
   "source": [
    "# Verify worker pods run in the SpotVM Nodes\n",
    "!kubectl get pods  -n elastic-job -o wide"
   ]
  },
  {
   "source": [
    "## Verify Training Logs\n",
    "Pods might stay in 'ContainerCreating' state  for a few minutes, while pulling the image ( image size is quite big and dockerhub is rate limiting). To optimize you could build container image and push it in ACR\n",
    "\n",
    "Get logs from workers and note how both workers joined the Rendezvous worker group with the same version:\n",
    "- imagenet-worker-0 pod `rendezvous version 1 as rank 0`\n",
    "- imagenet-worker-1 pod `rendezvous version 1 as rank 1`"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[INFO] 2021-05-25 19:42:09,129 launch: Running torchelastic.distributed.launch with args: ['/opt/conda/lib/python3.7/site-packages/torchelastic/distributed/launch.py', '--rdzv_backend=etcd', '--rdzv_endpoint=etcd-service:2379', '--rdzv_id=imagenet', '--nnodes=1:3', '--nproc_per_node=1', '/workspace/examples/imagenet/main.py', '--arch=resnet18', '--epochs=3', '--batch-size=64', '--workers=0', '/workspace/data/tiny-imagenet-200', '--checkpoint-file=/mnt/blob/data/checkpoint.pth.tar']\nINFO 2021-05-25 19:42:09,139 Etcd machines: ['http://0.0.0.0:2379']\n[INFO] 2021-05-25 19:42:09,149 launch: Using nproc_per_node=1.\n[INFO] 2021-05-25 19:42:09,878 api: [default] starting workers for function: wrapper_fn\n[INFO] 2021-05-25 19:42:09,879 api: [default] Rendezvous'ing worker group\nINFO 2021-05-25 19:42:09,879 Attempting to join next rendezvous\nINFO 2021-05-25 19:42:09,882 Observed existing rendezvous state: {'status': 'final', 'version': '2', 'participants': [0, 1, 2], 'keep_alives': ['/torchelastic/p2p/run_imagenet/rdzv/v_2/rank_1', '/torchelastic/p2p/run_imagenet/rdzv/v_2/rank_2', '/torchelastic/p2p/run_imagenet/rdzv/v_2/rank_0'], 'num_workers_waiting': 0}\nINFO 2021-05-25 19:42:09,904 Added self to waiting list. Rendezvous full state: {\"status\": \"final\", \"version\": \"2\", \"participants\": [0, 1, 2], \"keep_alives\": [\"/torchelastic/p2p/run_imagenet/rdzv/v_2/rank_1\", \"/torchelastic/p2p/run_imagenet/rdzv/v_2/rank_2\", \"/torchelastic/p2p/run_imagenet/rdzv/v_2/rank_0\"], \"num_workers_waiting\": 1}\nINFO 2021-05-25 19:42:09,906 Keep-alive key /torchelastic/p2p/run_imagenet/rdzv/v_2/rank_1 is not renewed.\nINFO 2021-05-25 19:42:09,906 Rendevous version 2 is incomplete. \nINFO 2021-05-25 19:42:09,906 Attempting to destroy it.\nINFO 2021-05-25 19:42:09,928 Destroyed rendezvous version 2 successfully.\nINFO 2021-05-25 19:42:09,928 Previously existing rendezvous state changed. Will re-try joining.\nINFO 2021-05-25 19:42:09,928 Attempting to join next rendezvous\nINFO 2021-05-25 19:42:09,951 New rendezvous state created: {'status': 'joinable', 'version': '3', 'participants': []}\nINFO 2021-05-25 19:42:09,971 Joined rendezvous version 3 as rank 0. Full state: {'status': 'joinable', 'version': '3', 'participants': [0]}\nINFO 2021-05-25 19:42:09,971 Rank 0 is responsible for join last call.\nINFO 2021-05-25 19:42:10,348 Rank 0 finished join last call.\nINFO 2021-05-25 19:42:10,348 Waiting for remaining peers.\nINFO 2021-05-25 19:42:10,349 All peers arrived. Confirming membership.\nINFO 2021-05-25 19:42:10,412 Waiting for confirmations from all peers.\nINFO 2021-05-25 19:42:10,427 Rendezvous version 3 is complete. Final state: {'status': 'final', 'version': '3', 'participants': [0, 1, 2], 'keep_alives': ['/torchelastic/p2p/run_imagenet/rdzv/v_3/rank_2', '/torchelastic/p2p/run_imagenet/rdzv/v_3/rank_0', '/torchelastic/p2p/run_imagenet/rdzv/v_3/rank_1'], 'num_workers_waiting': 0}\nINFO 2021-05-25 19:42:10,427 Creating EtcdStore as the c10d::Store implementation\n[INFO] 2021-05-25 19:42:10,439 api: [default] Rendezvous complete for workers.\nResult:\n\trestart_count=0\n\tgroup_rank=0\n\tgroup_world_size=3\n\trank stride=1\n\tassigned global_ranks=[0]\n\tmaster_addr=imagenet-worker-0\n\tmaster_port=46283\n\n[INFO] 2021-05-25 19:42:10,439 api: [default] Starting worker group\n=> set cuda device = 0\n=> creating model: resnet18\n=> no workers have checkpoints, starting from epoch 0\n=> start_epoch: 0, best_acc1: 0\nEpoch: [0][  0/521]\tTime  5.256 ( 5.256)\tData  0.136 ( 0.136)\tLoss 6.9013e+00 (6.9013e+00)\tAcc@1   3.12 (  3.12)\tAcc@5   4.69 (  4.69)\nEpoch: [0][ 10/521]\tTime  2.405 ( 2.542)\tData  0.099 ( 0.106)\tLoss 5.4396e+00 (6.1555e+00)\tAcc@1   0.00 (  0.43)\tAcc@5   3.12 (  3.55)\n"
     ]
    }
   ],
   "source": [
    "!kubectl logs imagenet-worker-0 -n elastic-job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[INFO] 2021-05-25 19:42:09,312 launch: Running torchelastic.distributed.launch with args: ['/opt/conda/lib/python3.7/site-packages/torchelastic/distributed/launch.py', '--rdzv_backend=etcd', '--rdzv_endpoint=etcd-service:2379', '--rdzv_id=imagenet', '--nnodes=1:3', '--nproc_per_node=1', '/workspace/examples/imagenet/main.py', '--arch=resnet18', '--epochs=3', '--batch-size=64', '--workers=0', '/workspace/data/tiny-imagenet-200', '--checkpoint-file=/mnt/blob/data/checkpoint.pth.tar']\nINFO 2021-05-25 19:42:09,320 Etcd machines: ['http://0.0.0.0:2379']\n[INFO] 2021-05-25 19:42:09,333 launch: Using nproc_per_node=1.\n[INFO] 2021-05-25 19:42:10,073 api: [default] starting workers for function: wrapper_fn\n[INFO] 2021-05-25 19:42:10,073 api: [default] Rendezvous'ing worker group\nINFO 2021-05-25 19:42:10,073 Attempting to join next rendezvous\nINFO 2021-05-25 19:42:10,078 Observed existing rendezvous state: {'status': 'joinable', 'version': '3', 'participants': [0]}\nINFO 2021-05-25 19:42:10,156 Joined rendezvous version 3 as rank 1. Full state: {'status': 'joinable', 'version': '3', 'participants': [0, 1]}\nINFO 2021-05-25 19:42:10,157 Waiting for remaining peers.\nINFO 2021-05-25 19:42:10,346 All peers arrived. Confirming membership.\nINFO 2021-05-25 19:42:10,424 Waiting for confirmations from all peers.\nINFO 2021-05-25 19:42:10,425 Rendezvous version 3 is complete. Final state: {'status': 'final', 'version': '3', 'participants': [0, 1, 2], 'keep_alives': ['/torchelastic/p2p/run_imagenet/rdzv/v_3/rank_2', '/torchelastic/p2p/run_imagenet/rdzv/v_3/rank_0', '/torchelastic/p2p/run_imagenet/rdzv/v_3/rank_1'], 'num_workers_waiting': 0}\nINFO 2021-05-25 19:42:10,425 Creating EtcdStore as the c10d::Store implementation\n[INFO] 2021-05-25 19:42:10,436 api: [default] Rendezvous complete for workers.\nResult:\n\trestart_count=0\n\tgroup_rank=1\n\tgroup_world_size=3\n\trank stride=1\n\tassigned global_ranks=[1]\n\tmaster_addr=imagenet-worker-0\n\tmaster_port=46283\n\n[INFO] 2021-05-25 19:42:10,436 api: [default] Starting worker group\n=> set cuda device = 0\n=> creating model: resnet18\n=> no workers have checkpoints, starting from epoch 0\n=> start_epoch: 0, best_acc1: 0\nEpoch: [0][  0/521]\tTime  5.263 ( 5.263)\tData  1.655 ( 1.655)\tLoss 7.0727e+00 (7.0727e+00)\tAcc@1   4.69 (  4.69)\tAcc@5   4.69 (  4.69)\nEpoch: [0][ 10/521]\tTime  2.409 ( 2.543)\tData  1.606 ( 1.485)\tLoss 5.5972e+00 (6.1422e+00)\tAcc@1   1.56 (  0.85)\tAcc@5   9.38 (  3.69)\n"
     ]
    }
   ],
   "source": [
    "!kubectl logs imagenet-worker-1 -n elastic-job"
   ]
  },
  {
   "source": [
    "## Checkpoint saved and restored\n",
    "Once Epoch training is completed you would see that training script saved the checkpoint in the Azure Blob storage. It takes about 10 min on  `Standard_NC12` node to run one Epoch iteration. **Stop** cell execution once logs indicate that checkpoint was saved after completing Epoch 0 training"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "=> creating model: resnet18\n",
      "=> loading checkpoint file: /mnt/blob/data/checkpoint.pth.tar\n",
      "=> loaded checkpoint file: /mnt/blob/data/checkpoint.pth.tar\n",
      "=> using checkpoint from rank: 1, max_epoch: 1\n",
      "=> checkpoint broadcast size is: 93588276\n",
      "=> done broadcasting checkpoint\n",
      "=> done restoring from previous checkpoint\n",
      "=> start_epoch: 2, best_acc1: 1.1100000143051147\n",
      "Epoch: [2][  0/782]\tTime  4.633 ( 4.633)\tData  1.505 ( 1.505)\tLoss 3.6480e+00 (3.6480e+00)\tAcc@1  18.75 ( 18.75)\tAcc@5  45.31 ( 45.31)\n",
      "Epoch: [2][ 10/782]\tTime  1.747 ( 2.258)\tData  1.363 ( 1.524)\tLoss 3.9434e+00 (3.8631e+00)\tAcc@1  14.06 ( 15.48)\tAcc@5  32.81 ( 35.51)\n",
      "=> loading checkpoint file: /mnt/blob/data/checkpoint.pth.tar\n",
      "=> loaded checkpoint file: /mnt/blob/data/checkpoint.pth.tar\n",
      "=> using checkpoint from rank: 1, max_epoch: 1\n",
      "=> checkpoint broadcast size is: 93588276\n",
      "/opt/conda/conda-bld/pytorch_1587428398394/work/torch/csrc/utils/tensor_numpy.cpp:141: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n",
      "=> done broadcasting checkpoint\n",
      "=> done restoring from previous checkpoint\n",
      "=> start_epoch: 2, best_acc1: 1.1100000143051147\n",
      "Epoch: [2][  0/782]\tTime  4.644 ( 4.644)\tData  1.422 ( 1.422)\tLoss 3.7847e+00 (3.7847e+00)\tAcc@1  17.19 ( 17.19)\tAcc@5  45.31 ( 45.31)\n",
      "Epoch: [2][ 10/782]\tTime  1.744 ( 2.257)\tData  0.862 ( 1.471)\tLoss 3.8525e+00 (3.9953e+00)\tAcc@1  10.94 ( 12.64)\tAcc@5  32.81 ( 34.52)\n",
      "Epoch: [2][ 20/782]\tTime  2.070 ( 2.035)\tData  1.264 ( 1.423)\tLoss 3.9437e+00 (3.9244e+00)\tAcc@1  17.19 ( 15.77)\tAcc@5  46.88 ( 35.86)\n",
      "Epoch: [2][ 20/782]\tTime  2.066 ( 2.035)\tData  1.668 ( 1.417)\tLoss 3.8652e+00 (3.9719e+00)\tAcc@1  14.06 ( 12.95)\tAcc@5  39.06 ( 34.52)\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    " # Stream logs from all workers until we see checkpoint is saved, then stop the Cell execution!!\n",
    " !kubectl logs -ljob-name=imagenet -n elastic-job -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "checkpoint.pth.tar\nmodel_best.pth.tar\n"
     ]
    }
   ],
   "source": [
    "# Verify Checkpoint and model file created by execing to pod \n",
    "!kubectl exec  -n elastic-job imagenet-worker-0 -- ls /mnt/blob/data"
   ]
  },
  {
   "source": [
    "Now that we have training running proceed to [Step4 Simulate Spot node Eviction](/Step4-SimulateStop.ipynb)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}