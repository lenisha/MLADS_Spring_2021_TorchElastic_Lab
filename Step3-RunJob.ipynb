{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit"
  },
  "interpreter": {
   "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Run Distributed Training Job\n",
    "In this notebook we will\n",
    "- Deploy Torch Elastic Kubernetes components \n",
    "  - Torch Elastic Operator\n",
    "  - ETCD server for training control plane\n",
    "- Prepare and Deploy `ElasticJob` based on PyTorch Elastic ImageNet Training  \n",
    "- Validate Training is running on multiple GPU workers"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Deploy Torch Elastic Kubernetes components\n",
    "\n",
    "## ETCD server\n",
    "First we will install ETCD server whcih will act as Rendezevous server orchestrating training workers.\n",
    "In this example we use simple ETCD pod/service deployment, for production use Helm chart that deploy `ETCD` in HA mode."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "service/etcd-service unchanged\n",
      "pod/etcd unchanged\n",
      "NAME           TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)    AGE\n",
      "etcd-service   ClusterIP   10.0.216.198   <none>        2379/TCP   42s\n",
      "NAME   READY   STATUS    RESTARTS   AGE   IP            NODE                                 NOMINATED NODE   READINESS GATES\n",
      "etcd   1/1     Running   0          43s   10.244.12.2   aks-cpuworkers-40607851-vmss000000   <none>           <none>\n"
     ]
    }
   ],
   "source": [
    "!kubectl apply -f kube/etcd.yaml\n",
    "# verify service and pods scheduled on CPU nodes\n",
    "!kubectl get svc -n elastic-job\n",
    "!kubectl get pods -n elastic-job -o wide\n"
   ]
  },
  {
   "source": [
    "## Deploy TorchElastic Operator\n",
    "Details and Kubernetes manifests descibed at https://github.com/pytorch/elastic/tree/master/kubernetes, we have included manifests in the repo for simplicity:\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Warning: resource namespaces/elastic-job is missing the kubectl.kubernetes.io/last-applied-configuration annotation which is required by kubectl apply. kubectl apply should only be used on resources created declaratively by either kubectl create --save-config or kubectl apply. The missing annotation will be patched automatically.\n",
      "namespace/elastic-job configured\n",
      "customresourcedefinition.apiextensions.k8s.io/elasticjobs.elastic.pytorch.org created\n",
      "role.rbac.authorization.k8s.io/leader-election-role created\n",
      "clusterrole.rbac.authorization.k8s.io/elastic-job-k8s-controller-role created\n",
      "rolebinding.rbac.authorization.k8s.io/leader-election-rolebinding created\n",
      "clusterrolebinding.rbac.authorization.k8s.io/elastic-job-k8s-controller-rolebinding created\n",
      "deployment.apps/elastic-job-k8s-controller created\n"
     ]
    }
   ],
   "source": [
    "!kubectl apply -k kube/config/default"
   ]
  },
  {
   "source": [
    "# Verify that the ElasticJob custom resource is installed\n",
    "!kubectl get crd\n"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 6,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "NAME                              CREATED AT\nelasticjobs.elastic.pytorch.org   2021-05-24T00:48:59Z\n"
     ]
    }
   ]
  },
  {
   "source": [
    "## Prepare ElasticJob Deployment\n",
    "### Training Docker imagen\n",
    "Take a look at `Dockerfile` for our deployment, it is in [examples/Dockerfile](examples/Dockerfile).\n",
    "It is based on `pytorch` gpu enabled image and has both training script `main.py` and dataset for training and testing as part of it.\n",
    "In production dataset should reside in blob and deployment should point to it.\n",
    "\n",
    "Keep a not theta entrypoint in the container is launch\n",
    "```\n",
    "ENTRYPOINT [\"python\", \"-m\", \"torch.distributed.run\"]\n",
    "```\n",
    "For more details refer to https://github.com/pytorch/elastic/tree/master/examples\n",
    "\n",
    "## Kubernetes Job config\n",
    "We have updated Kubernetes `ElasticJob` manifest [kube/imagenet.yaml](kube/imagenet.yaml) to make sure it could run on Spot nodes and mount Azure Blob for saving checkpoints.\n",
    "\n",
    "- Note we are deploying Custom Kubernetes Resource `ElasticJob` that Torch Elastic operator will process and orchestrate\n",
    "- We pointed `rdzvEndpoint` to previously deployed ETCD service \n",
    "- Note min/max number of replicas that directs training job on number of desired workers, if you increase **number of workers** you would see that each worker is performing training on smaller subset of data and overall job completes much faster\n",
    "\n",
    "```yaml\n",
    "  apiVersion: elastic.pytorch.org/v1alpha1\n",
    "  kind: ElasticJob\n",
    "  metadata:\n",
    "    name: imagenet\n",
    "    namespace: elastic-job\n",
    "  spec:\n",
    "    # Use \"etcd-service:2379\" if you already apply etcd.yaml\n",
    "    rdzvEndpoint: \"etcd-service:2379\"\n",
    "    minReplicas: 1\n",
    "    maxReplicas: 2    \n",
    "    replicaSpecs:\n",
    "      Worker:\n",
    "        replicas: 2\n",
    "```\n",
    "\n",
    "- Updated `kube/imagenet.yaml` with **tolerations** and **nodeSelector** to run training on Spot VM nodepool\n",
    "\n",
    "```yaml\n",
    "    containers:\n",
    "    - name: elasticjob-worker\n",
    "      image: torchelastic/examples:0.2.0\n",
    "      imagePullPolicy: Always\n",
    "       ..\n",
    "    nodeSelector:\n",
    "       kubernetes.azure.com/scalesetpriority: spot\n",
    "    tolerations:\n",
    "    - key: \"kubernetes.azure.com/scalesetpriority\"\n",
    "      operator: \"Equal\"\n",
    "      value: \"spot\"\n",
    "      effect: \"NoSchedule\"       \n",
    "```\n",
    "\n",
    "- Updated `kube/imagenet.yaml` with **volumes** and **volumemount** to provide storage to the training job to save checkpoint to the path set in arguments `--checkpoint-file`\n",
    "\n",
    "```yaml\n",
    " volumes:  \n",
    " - name: trainingdata\n",
    "   persistentVolumeClaim:\n",
    "      claimName: pvc-blob\n",
    " containers:\n",
    "  ...\n",
    "  args:\n",
    "   - \"--nproc_per_node=1\"\n",
    "   - \"/workspace/examples/imagenet/main.py\"\n",
    "   - \"--arch=resnet18\"\n",
    "   - \"--epochs=3\"\n",
    "   - \"--batch-size=64\"\n",
    "   - \"--workers=0\"\n",
    "   - \"/workspace/data/tiny-imagenet-200\"\n",
    "   - \"--checkpoint-file=/mnt/blob/data/checkpoint.pth.tar\"\n",
    "  volumeMounts:\n",
    "  - name: trainingdata\n",
    "    mountPath: \"/mnt/blob/data\"         \n",
    "```\n",
    "\n",
    "- Note in the arguments `--nproc_per_node` directs on how many local pytorch workers could run per node (typically is equal to number of CUDA devices), '--workers` is number of workers in Pytorch Dataloader \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "elasticjob.elastic.pytorch.org/imagenet configured\n",
      "Name:         imagenet\n",
      "Namespace:    elastic-job\n",
      "Labels:       <none>\n",
      "Annotations:  <none>\n",
      "API Version:  elastic.pytorch.org/v1alpha1\n",
      "Kind:         ElasticJob\n",
      "Metadata:\n",
      "  Creation Timestamp:  2021-05-24T02:53:56Z\n",
      "  Generation:          3\n",
      "  Managed Fields:\n",
      "    API Version:  elastic.pytorch.org/v1alpha1\n",
      "    Fields Type:  FieldsV1\n",
      "    fieldsV1:\n",
      "      f:spec:\n",
      "        f:RunPolicy:\n",
      "          .:\n",
      "          f:cleanPodPolicy:\n",
      "        f:replicaSpecs:\n",
      "          f:Worker:\n",
      "            f:template:\n",
      "              f:metadata:\n",
      "                .:\n",
      "                f:creationTimestamp:\n",
      "      f:status:\n",
      "        .:\n",
      "        f:conditions:\n",
      "        f:replicaStatuses:\n",
      "          .:\n",
      "          f:Worker:\n",
      "    Manager:      manager\n",
      "    Operation:    Update\n",
      "    Time:         2021-05-24T02:53:57Z\n",
      "    API Version:  elastic.pytorch.org/v1alpha1\n",
      "    Fields Type:  FieldsV1\n",
      "    fieldsV1:\n",
      "      f:metadata:\n",
      "        f:annotations:\n",
      "          .:\n",
      "          f:kubectl.kubernetes.io/last-applied-configuration:\n",
      "      f:spec:\n",
      "        .:\n",
      "        f:maxReplicas:\n",
      "        f:minReplicas:\n",
      "        f:rdzvEndpoint:\n",
      "        f:replicaSpecs:\n",
      "          .:\n",
      "          f:Worker:\n",
      "            .:\n",
      "            f:replicas:\n",
      "            f:restartPolicy:\n",
      "            f:template:\n",
      "              .:\n",
      "              f:apiVersion:\n",
      "              f:kind:\n",
      "              f:spec:\n",
      "                .:\n",
      "                f:containers:\n",
      "                f:nodeSelector:\n",
      "                  .:\n",
      "                  f:kubernetes.azure.com/scalesetpriority:\n",
      "                f:tolerations:\n",
      "                f:volumes:\n",
      "    Manager:         kubectl-client-side-apply\n",
      "    Operation:       Update\n",
      "    Time:            2021-05-24T02:55:04Z\n",
      "  Resource Version:  73454\n",
      "  Self Link:         /apis/elastic.pytorch.org/v1alpha1/namespaces/elastic-job/elasticjobs/imagenet\n",
      "  UID:               46692bc0-c3c9-4530-a875-8db7fd98624c\n",
      "Spec:\n",
      "  Run Policy:\n",
      "    Clean Pod Policy:  None\n",
      "  Max Replicas:        2\n",
      "  Min Replicas:        1\n",
      "  Rdzv Endpoint:       etcd-service:2379\n",
      "  Replica Specs:\n",
      "    Worker:\n",
      "      Replicas:        2\n",
      "      Restart Policy:  ExitCode\n",
      "      Template:\n",
      "        API Version:  v1\n",
      "        Kind:         Pod\n",
      "        Metadata:\n",
      "          Creation Timestamp:  <nil>\n",
      "        Spec:\n",
      "          Containers:\n",
      "            Args:\n",
      "              --nproc_per_node=1\n",
      "              /workspace/examples/imagenet/main.py\n",
      "              --arch=resnet18\n",
      "              --epochs=3\n",
      "              --batch-size=64\n",
      "              --workers=0\n",
      "              /workspace/data/tiny-imagenet-200\n",
      "              --checkpoint-file=/mnt/blob/data/checkpoint.pth.tar\n",
      "            Image:              torchelastic/examples:0.2.0\n",
      "            Image Pull Policy:  Always\n",
      "            Name:               elasticjob-worker\n",
      "            Resources:\n",
      "              Limits:\n",
      "                nvidia.com/gpu:  1\n",
      "            Volume Mounts:\n",
      "              Mount Path:  /mnt/blob/data\n",
      "              Name:        trainingdata\n",
      "          Node Selector:\n",
      "            kubernetes.azure.com/scalesetpriority:  spot\n",
      "          Tolerations:\n",
      "            Effect:    NoSchedule\n",
      "            Key:       kubernetes.azure.com/scalesetpriority\n",
      "            Operator:  Equal\n",
      "            Value:     spot\n",
      "          Volumes:\n",
      "            Name:  trainingdata\n",
      "            Persistent Volume Claim:\n",
      "              Claim Name:  pvc-blob\n",
      "Status:\n",
      "  Conditions:\n",
      "    Last Transition Time:  2021-05-24T02:53:56Z\n",
      "    Last Update Time:      2021-05-24T02:53:56Z\n",
      "    Message:               ElasticJob imagenet is running.\n",
      "    Reason:                ElasticJobRunning\n",
      "    Status:                True\n",
      "    Type:                  Running\n",
      "  Replica Statuses:\n",
      "    Worker:\n",
      "Events:\n",
      "  Type    Reason                   Age   From                    Message\n",
      "  ----    ------                   ----  ----                    -------\n",
      "  Normal  SuccessfulCreatePod      67s   elastic-job-controller  Created pod: imagenet-worker-0\n",
      "  Normal  SuccessfulCreatePod      67s   elastic-job-controller  Created pod: imagenet-worker-1\n",
      "  Normal  SuccessfulCreateService  67s   elastic-job-controller  Created service: imagenet-worker-0\n",
      "  Normal  SuccessfulCreateService  67s   elastic-job-controller  Created service: imagenet-worker-1\n"
     ]
    }
   ],
   "source": [
    "# Run the deployment\n",
    "!kubectl apply -f kube/imagenet.yaml\n",
    "# Verify ElasticJob\n",
    "!kubectl describe elasticjob -n elastic-job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['NAME                                          READY   STATUS    RESTARTS   AGE     IP            NODE                                 NOMINATED NODE   READINESS GATES',\n",
       " 'elastic-job-k8s-controller-5b9bc6b79c-xvdsw   1/1     Running   0          129m    10.244.13.2   aks-cpuworkers-40607851-vmss000003   <none>           <none>',\n",
       " 'etcd                                          1/1     Running   0          134m    10.244.12.2   aks-cpuworkers-40607851-vmss000000   <none>           <none>',\n",
       " 'imagenet-worker-0                             1/1     Running   0          4m27s   10.244.7.3    aks-spotgpu-40607851-vmss000001      <none>           <none>',\n",
       " 'imagenet-worker-1                             1/1     Running   0          4m27s   10.244.7.2    aks-spotgpu-40607851-vmss000001      <none>           <none>']"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "# Verify worker pods run in the SpotVM Nodes\n",
    "!!kubectl get pods  -n elastic-job -o wide"
   ]
  },
  {
   "source": [
    "## Verify Training Logs\n",
    "Pods might stay in 'ContainerCreating' state  for a few minutes, while pulling the image ( image size is quite big and dockerhub is rate limiting). To optimize you could build container image and push it in ACR\n",
    "\n",
    "Get logs from workers and note how both workers joined the Rendezvous worker group with the same version:\n",
    "- imagenet-worker-0 pod `rendezvous version 1 as rank 0`\n",
    "- imagenet-worker-1 pod `rendezvous version 1 as rank 1`"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[INFO] 2021-05-24 02:56:48,499 launch: Running torchelastic.distributed.launch with args: ['/opt/conda/lib/python3.7/site-packages/torchelastic/distributed/launch.py', '--rdzv_backend=etcd', '--rdzv_endpoint=etcd-service:2379', '--rdzv_id=imagenet', '--nnodes=1:2', '--nproc_per_node=1', '/workspace/examples/imagenet/main.py', '--arch=resnet18', '--epochs=3', '--batch-size=64', '--workers=0', '/workspace/data/tiny-imagenet-200', '--checkpoint-file=/mnt/blob/data/checkpoint.pth.tar']\nINFO 2021-05-24 02:56:48,505 Etcd machines: ['http://0.0.0.0:2379']\n[INFO] 2021-05-24 02:56:48,516 launch: Using nproc_per_node=1.\n[INFO] 2021-05-24 02:56:49,239 api: [default] starting workers for function: wrapper_fn\n[INFO] 2021-05-24 02:56:49,239 api: [default] Rendezvous'ing worker group\nINFO 2021-05-24 02:56:49,239 Attempting to join next rendezvous\nINFO 2021-05-24 02:56:49,266 New rendezvous state created: {'status': 'joinable', 'version': '1', 'participants': []}\nINFO 2021-05-24 02:56:49,299 Joined rendezvous version 1 as rank 0. Full state: {'status': 'joinable', 'version': '1', 'participants': [0]}\nINFO 2021-05-24 02:56:49,299 Rank 0 is responsible for join last call.\nINFO 2021-05-24 02:56:49,321 Rank 0 finished join last call.\nINFO 2021-05-24 02:56:49,321 Waiting for remaining peers.\nINFO 2021-05-24 02:56:49,322 All peers arrived. Confirming membership.\nINFO 2021-05-24 02:56:49,383 Waiting for confirmations from all peers.\nINFO 2021-05-24 02:56:49,384 Rendezvous version 1 is complete. Final state: {'status': 'final', 'version': '1', 'participants': [0, 1], 'keep_alives': ['/torchelastic/p2p/run_imagenet/rdzv/v_1/rank_1', '/torchelastic/p2p/run_imagenet/rdzv/v_1/rank_0'], 'num_workers_waiting': 0}\nINFO 2021-05-24 02:56:49,384 Creating EtcdStore as the c10d::Store implementation\n[INFO] 2021-05-24 02:56:49,392 api: [default] Rendezvous complete for workers.\nResult:\n\trestart_count=0\n\tgroup_rank=0\n\tgroup_world_size=2\n\trank stride=1\n\tassigned global_ranks=[0]\n\tmaster_addr=imagenet-worker-0\n\tmaster_port=40119\n\n[INFO] 2021-05-24 02:56:49,392 api: [default] Starting worker group\n=> set cuda device = 0\n=> creating model: resnet18\n=> no workers have checkpoints, starting from epoch 0\n=> start_epoch: 0, best_acc1: 0\nEpoch: [0][  0/782]\tTime  4.642 ( 4.642)\tData  1.555 ( 1.555)\tLoss 7.0832e+00 (7.0832e+00)\tAcc@1   1.56 (  1.56)\tAcc@5   3.12 (  3.12)\nEpoch: [0][ 10/782]\tTime  1.792 ( 2.031)\tData  1.150 ( 1.336)\tLoss 5.7643e+00 (6.2991e+00)\tAcc@1   0.00 (  0.71)\tAcc@5   1.56 (  2.84)\nEpoch: [0][ 20/782]\tTime  2.094 ( 1.943)\tData  1.629 ( 1.368)\tLoss 6.0132e+00 (6.0899e+00)\tAcc@1   0.00 (  0.97)\tAcc@5   6.25 (  4.02)\nEpoch: [0][ 30/782]\tTime  1.729 ( 1.902)\tData  1.343 ( 1.374)\tLoss 5.3881e+00 (5.9217e+00)\tAcc@1   1.56 (  0.96)\tAcc@5  17.19 (  4.54)\nEpoch: [0][ 40/782]\tTime  1.680 ( 1.850)\tData  1.298 ( 1.351)\tLoss 5.5564e+00 (5.8248e+00)\tAcc@1   1.56 (  1.07)\tAcc@5   4.69 (  4.80)\nEpoch: [0][ 50/782]\tTime  1.382 ( 1.791)\tData  0.984 ( 1.301)\tLoss 5.2530e+00 (5.7221e+00)\tAcc@1   0.00 (  1.23)\tAcc@5   3.12 (  5.36)\nEpoch: [0][ 60/782]\tTime  1.755 ( 1.762)\tData  1.118 ( 1.277)\tLoss 5.3787e+00 (5.6532e+00)\tAcc@1   0.00 (  1.31)\tAcc@5   9.38 (  5.81)\nEpoch: [0][ 70/782]\tTime  1.895 ( 1.755)\tData  1.508 ( 1.273)\tLoss 5.3182e+00 (5.6036e+00)\tAcc@1   3.12 (  1.43)\tAcc@5   6.25 (  5.96)\nEpoch: [0][ 80/782]\tTime  1.371 ( 1.750)\tData  0.969 ( 1.272)\tLoss 5.3637e+00 (5.5651e+00)\tAcc@1   0.00 (  1.39)\tAcc@5   6.25 (  5.84)\nEpoch: [0][ 90/782]\tTime  1.934 ( 1.744)\tData  1.332 ( 1.267)\tLoss 5.3016e+00 (5.5189e+00)\tAcc@1   1.56 (  1.51)\tAcc@5   3.12 (  6.18)\nEpoch: [0][100/782]\tTime  1.429 ( 1.742)\tData  0.894 ( 1.269)\tLoss 5.0583e+00 (5.4810e+00)\tAcc@1   1.56 (  1.70)\tAcc@5   7.81 (  6.51)\nEpoch: [0][110/782]\tTime  1.697 ( 1.732)\tData  1.144 ( 1.266)\tLoss 5.0432e+00 (5.4459e+00)\tAcc@1   4.69 (  1.89)\tAcc@5   9.38 (  6.70)\nEpoch: [0][120/782]\tTime  2.131 ( 1.749)\tData  1.738 ( 1.284)\tLoss 5.1071e+00 (5.4145e+00)\tAcc@1   0.00 (  1.92)\tAcc@5   7.81 (  6.99)\nEpoch: [0][130/782]\tTime  1.725 ( 1.747)\tData  1.343 ( 1.283)\tLoss 5.1234e+00 (5.3915e+00)\tAcc@1   3.12 (  1.93)\tAcc@5  12.50 (  7.16)\nEpoch: [0][140/782]\tTime  1.477 ( 1.749)\tData  1.095 ( 1.287)\tLoss 4.9438e+00 (5.3673e+00)\tAcc@1   0.00 (  1.99)\tAcc@5  10.94 (  7.52)\n"
     ]
    }
   ],
   "source": [
    "!kubectl logs imagenet-worker-0 -n elastic-job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[INFO] 2021-05-24 02:56:48,497 launch: Running torchelastic.distributed.launch with args: ['/opt/conda/lib/python3.7/site-packages/torchelastic/distributed/launch.py', '--rdzv_backend=etcd', '--rdzv_endpoint=etcd-service:2379', '--rdzv_id=imagenet', '--nnodes=1:2', '--nproc_per_node=1', '/workspace/examples/imagenet/main.py', '--arch=resnet18', '--epochs=3', '--batch-size=64', '--workers=0', '/workspace/data/tiny-imagenet-200', '--checkpoint-file=/mnt/blob/data/checkpoint.pth.tar']\nINFO 2021-05-24 02:56:48,505 Etcd machines: ['http://0.0.0.0:2379']\n[INFO] 2021-05-24 02:56:48,517 launch: Using nproc_per_node=1.\n[INFO] 2021-05-24 02:56:49,268 api: [default] starting workers for function: wrapper_fn\n[INFO] 2021-05-24 02:56:49,268 api: [default] Rendezvous'ing worker group\nINFO 2021-05-24 02:56:49,268 Attempting to join next rendezvous\nINFO 2021-05-24 02:56:49,272 Observed existing rendezvous state: {'status': 'joinable', 'version': '1', 'participants': []}\nINFO 2021-05-24 02:56:49,320 Joined rendezvous version 1 as rank 1. Full state: {'status': 'frozen', 'version': '1', 'participants': [0, 1], 'keep_alives': []}\nINFO 2021-05-24 02:56:49,320 Waiting for remaining peers.\nINFO 2021-05-24 02:56:49,321 All peers arrived. Confirming membership.\nINFO 2021-05-24 02:56:49,364 Waiting for confirmations from all peers.\nINFO 2021-05-24 02:56:49,383 Rendezvous version 1 is complete. Final state: {'status': 'final', 'version': '1', 'participants': [0, 1], 'keep_alives': ['/torchelastic/p2p/run_imagenet/rdzv/v_1/rank_1', '/torchelastic/p2p/run_imagenet/rdzv/v_1/rank_0'], 'num_workers_waiting': 0}\nINFO 2021-05-24 02:56:49,383 Creating EtcdStore as the c10d::Store implementation\n[INFO] 2021-05-24 02:56:49,391 api: [default] Rendezvous complete for workers.\nResult:\n\trestart_count=0\n\tgroup_rank=1\n\tgroup_world_size=2\n\trank stride=1\n\tassigned global_ranks=[1]\n\tmaster_addr=imagenet-worker-0\n\tmaster_port=40119\n\n[INFO] 2021-05-24 02:56:49,391 api: [default] Starting worker group\n=> set cuda device = 0\n=> creating model: resnet18\n=> no workers have checkpoints, starting from epoch 0\n=> start_epoch: 0, best_acc1: 0\nEpoch: [0][  0/782]\tTime  4.641 ( 4.641)\tData  1.309 ( 1.309)\tLoss 7.1585e+00 (7.1585e+00)\tAcc@1   0.00 (  0.00)\tAcc@5   0.00 (  0.00)\nEpoch: [0][ 10/782]\tTime  1.773 ( 2.029)\tData  1.322 ( 1.320)\tLoss 5.9893e+00 (6.3748e+00)\tAcc@1   0.00 (  0.14)\tAcc@5   1.56 (  2.13)\nEpoch: [0][ 20/782]\tTime  2.101 ( 1.943)\tData  1.520 ( 1.320)\tLoss 5.5654e+00 (6.0636e+00)\tAcc@1   3.12 (  0.60)\tAcc@5   7.81 (  3.12)\nEpoch: [0][ 30/782]\tTime  1.731 ( 1.902)\tData  1.215 ( 1.345)\tLoss 5.4684e+00 (5.9096e+00)\tAcc@1   0.00 (  0.91)\tAcc@5   1.56 (  3.58)\nEpoch: [0][ 40/782]\tTime  1.677 ( 1.850)\tData  1.088 ( 1.299)\tLoss 5.3787e+00 (5.8147e+00)\tAcc@1   1.56 (  0.99)\tAcc@5   7.81 (  3.81)\nEpoch: [0][ 50/782]\tTime  1.387 ( 1.791)\tData  1.001 ( 1.257)\tLoss 5.3241e+00 (5.7370e+00)\tAcc@1   3.12 (  0.98)\tAcc@5   9.38 (  3.92)\nEpoch: [0][ 60/782]\tTime  1.758 ( 1.762)\tData  1.374 ( 1.246)\tLoss 5.3525e+00 (5.6690e+00)\tAcc@1   1.56 (  1.18)\tAcc@5   6.25 (  4.28)\nEpoch: [0][ 70/782]\tTime  1.891 ( 1.755)\tData  1.453 ( 1.252)\tLoss 5.2449e+00 (5.6156e+00)\tAcc@1   3.12 (  1.34)\tAcc@5  10.94 (  4.78)\nEpoch: [0][ 80/782]\tTime  1.364 ( 1.750)\tData  0.970 ( 1.252)\tLoss 5.0373e+00 (5.5664e+00)\tAcc@1   1.56 (  1.35)\tAcc@5   7.81 (  5.15)\nEpoch: [0][ 90/782]\tTime  1.945 ( 1.744)\tData  1.550 ( 1.245)\tLoss 5.0951e+00 (5.5144e+00)\tAcc@1   3.12 (  1.51)\tAcc@5   9.38 (  5.46)\nEpoch: [0][100/782]\tTime  1.435 ( 1.742)\tData  1.045 ( 1.244)\tLoss 5.2322e+00 (5.4824e+00)\tAcc@1   1.56 (  1.58)\tAcc@5   1.56 (  5.62)\nEpoch: [0][110/782]\tTime  1.709 ( 1.732)\tData  1.322 ( 1.231)\tLoss 5.1802e+00 (5.4548e+00)\tAcc@1   1.56 (  1.63)\tAcc@5   4.69 (  5.81)\nEpoch: [0][120/782]\tTime  2.115 ( 1.749)\tData  1.610 ( 1.245)\tLoss 5.1923e+00 (5.4223e+00)\tAcc@1   0.00 (  1.73)\tAcc@5   9.38 (  6.24)\nEpoch: [0][130/782]\tTime  1.724 ( 1.747)\tData  1.254 ( 1.248)\tLoss 4.9842e+00 (5.3974e+00)\tAcc@1   1.56 (  1.77)\tAcc@5  10.94 (  6.64)\nEpoch: [0][140/782]\tTime  1.478 ( 1.749)\tData  1.072 ( 1.255)\tLoss 5.0428e+00 (5.3738e+00)\tAcc@1   0.00 (  1.77)\tAcc@5  10.94 (  6.91)\nEpoch: [0][150/782]\tTime  1.627 ( 1.746)\tData  0.763 ( 1.251)\tLoss 4.8531e+00 (5.3514e+00)\tAcc@1   4.69 (  1.83)\tAcc@5  10.94 (  7.20)\nEpoch: [0][160/782]\tTime  1.445 ( 1.734)\tData  1.060 ( 1.242)\tLoss 5.2246e+00 (5.3334e+00)\tAcc@1   3.12 (  1.88)\tAcc@5   7.81 (  7.31)\nEpoch: [0][170/782]\tTime  2.068 ( 1.728)\tData  1.680 ( 1.242)\tLoss 4.8414e+00 (5.3159e+00)\tAcc@1   3.12 (  1.99)\tAcc@5  12.50 (  7.59)\nEpoch: [0][180/782]\tTime  0.996 ( 1.715)\tData  0.606 ( 1.232)\tLoss 5.0757e+00 (5.3001e+00)\tAcc@1   0.00 (  1.99)\tAcc@5   7.81 (  7.83)\nEpoch: [0][190/782]\tTime  1.862 ( 1.717)\tData  1.132 ( 1.236)\tLoss 4.9643e+00 (5.2847e+00)\tAcc@1   3.12 (  2.07)\tAcc@5  10.94 (  8.02)\nEpoch: [0][200/782]\tTime  1.672 ( 1.715)\tData  1.074 ( 1.234)\tLoss 4.9920e+00 (5.2700e+00)\tAcc@1   1.56 (  2.11)\tAcc@5   9.38 (  8.19)\nEpoch: [0][210/782]\tTime  1.942 ( 1.716)\tData  1.221 ( 1.234)\tLoss 4.6788e+00 (5.2508e+00)\tAcc@1   6.25 (  2.24)\tAcc@5  25.00 (  8.53)\nEpoch: [0][220/782]\tTime  1.487 ( 1.713)\tData  1.105 ( 1.231)\tLoss 4.9615e+00 (5.2339e+00)\tAcc@1   4.69 (  2.31)\tAcc@5  14.06 (  8.87)\nEpoch: [0][230/782]\tTime  1.773 ( 1.708)\tData  1.039 ( 1.225)\tLoss 4.9605e+00 (5.2187e+00)\tAcc@1   4.69 (  2.37)\tAcc@5  12.50 (  9.15)\nEpoch: [0][240/782]\tTime  1.822 ( 1.710)\tData  1.440 ( 1.228)\tLoss 5.0251e+00 (5.2027e+00)\tAcc@1   6.25 (  2.48)\tAcc@5  10.94 (  9.35)\nEpoch: [0][250/782]\tTime  1.661 ( 1.710)\tData  1.275 ( 1.229)\tLoss 4.8962e+00 (5.1933e+00)\tAcc@1   3.12 (  2.53)\tAcc@5  12.50 (  9.49)\nEpoch: [0][260/782]\tTime  2.010 ( 1.709)\tData  1.627 ( 1.228)\tLoss 4.8617e+00 (5.1794e+00)\tAcc@1   1.56 (  2.58)\tAcc@5  15.62 (  9.70)\nEpoch: [0][270/782]\tTime  1.483 ( 1.708)\tData  1.097 ( 1.228)\tLoss 4.8870e+00 (5.1676e+00)\tAcc@1   1.56 (  2.63)\tAcc@5  17.19 (  9.92)\nEpoch: [0][280/782]\tTime  1.553 ( 1.706)\tData  1.156 ( 1.228)\tLoss 4.9124e+00 (5.1573e+00)\tAcc@1   3.12 (  2.70)\tAcc@5  15.62 ( 10.07)\nEpoch: [0][290/782]\tTime  1.337 ( 1.698)\tData  0.687 ( 1.221)\tLoss 4.8078e+00 (5.1443e+00)\tAcc@1   3.12 (  2.79)\tAcc@5  12.50 ( 10.34)\nEpoch: [0][300/782]\tTime  1.699 ( 1.700)\tData  1.030 ( 1.218)\tLoss 4.6284e+00 (5.1335e+00)\tAcc@1   6.25 (  2.88)\tAcc@5  23.44 ( 10.52)\n"
     ]
    }
   ],
   "source": [
    "!kubectl logs imagenet-worker-1 -n elastic-job"
   ]
  },
  {
   "source": [
    "## Checkpoint saved and restored\n",
    "Once Epoch training is completed you would see that training script saved the checkpoint in the Azure Blob storage. It takes about 10 min on  `Standard_NC12` node to run one Epoch iteration"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "=> creating model: resnet18\n",
      "=> loading checkpoint file: /mnt/blob/data/checkpoint.pth.tar\n",
      "=> loaded checkpoint file: /mnt/blob/data/checkpoint.pth.tar\n",
      "=> using checkpoint from rank: 1, max_epoch: 1\n",
      "=> checkpoint broadcast size is: 93588276\n",
      "=> done broadcasting checkpoint\n",
      "=> done restoring from previous checkpoint\n",
      "=> start_epoch: 2, best_acc1: 1.1100000143051147\n",
      "Epoch: [2][  0/782]\tTime  4.633 ( 4.633)\tData  1.505 ( 1.505)\tLoss 3.6480e+00 (3.6480e+00)\tAcc@1  18.75 ( 18.75)\tAcc@5  45.31 ( 45.31)\n",
      "Epoch: [2][ 10/782]\tTime  1.747 ( 2.258)\tData  1.363 ( 1.524)\tLoss 3.9434e+00 (3.8631e+00)\tAcc@1  14.06 ( 15.48)\tAcc@5  32.81 ( 35.51)\n",
      "=> loading checkpoint file: /mnt/blob/data/checkpoint.pth.tar\n",
      "=> loaded checkpoint file: /mnt/blob/data/checkpoint.pth.tar\n",
      "=> using checkpoint from rank: 1, max_epoch: 1\n",
      "=> checkpoint broadcast size is: 93588276\n",
      "/opt/conda/conda-bld/pytorch_1587428398394/work/torch/csrc/utils/tensor_numpy.cpp:141: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n",
      "=> done broadcasting checkpoint\n",
      "=> done restoring from previous checkpoint\n",
      "=> start_epoch: 2, best_acc1: 1.1100000143051147\n",
      "Epoch: [2][  0/782]\tTime  4.644 ( 4.644)\tData  1.422 ( 1.422)\tLoss 3.7847e+00 (3.7847e+00)\tAcc@1  17.19 ( 17.19)\tAcc@5  45.31 ( 45.31)\n",
      "Epoch: [2][ 10/782]\tTime  1.744 ( 2.257)\tData  0.862 ( 1.471)\tLoss 3.8525e+00 (3.9953e+00)\tAcc@1  10.94 ( 12.64)\tAcc@5  32.81 ( 34.52)\n",
      "Epoch: [2][ 20/782]\tTime  2.070 ( 2.035)\tData  1.264 ( 1.423)\tLoss 3.9437e+00 (3.9244e+00)\tAcc@1  17.19 ( 15.77)\tAcc@5  46.88 ( 35.86)\n",
      "Epoch: [2][ 20/782]\tTime  2.066 ( 2.035)\tData  1.668 ( 1.417)\tLoss 3.8652e+00 (3.9719e+00)\tAcc@1  14.06 ( 12.95)\tAcc@5  39.06 ( 34.52)\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    " # Stream logs from all workers until we see checkpoint is saved, then stop the Cell execution!!\n",
    " !kubectl logs -ljob-name=imagenet -n elastic-job -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "checkpoint.pth.tar\nmodel_best.pth.tar\n"
     ]
    }
   ],
   "source": [
    "# Verify Checkpoint and model file created by execing to pod \n",
    "!kubectl exec  -n elastic-job imagenet-worker-0 -- ls /mnt/blob/data"
   ]
  },
  {
   "source": [
    "Now that we have training running proceed to [Step4 Simulate Spot node Eviction](/Step4-SimulateStop.ipynb)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}